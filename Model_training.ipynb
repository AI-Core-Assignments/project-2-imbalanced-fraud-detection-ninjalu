{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# import the data and preprocess\n",
    "train_data = pd.read_csv('creditcard_train.csv')\n",
    "test_data = pd.read_csv('creditcard_test.csv')\n",
    "\n",
    "# preprocessing as per outlined in the EDA section\n",
    "X, y= preprocessing.preproc(train_data)\n",
    "X_test, y_test = preprocessing.preproc(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic logistic regression\n",
    "We start with a simple logistic regression model as a baseline. <br> \n",
    "We use logistic regression hyperparameter C=100 (controlling the power of L2 regularisation)\n",
    "We will try different sampling methods, without sampling, BorderlineSMOTE, random undersampling, BorderlineSMOTE with random undersampling. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, classification_report\n",
    "from metrics import pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lr_model(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    model = LogisticRegression(max_iter=1e4, C=1e2)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pre = model.predict(X_train)\n",
    "    y_val_pre = model.predict(X_val)\n",
    "    y_test_pre = model.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_train, y_train_pre))\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(classification_report(y_val, y_val_pre))\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(classification_report(y_test, y_test_pre))\n",
    "    print('---------------------------------------------------------------------')\n",
    "\n",
    "    print('Train pr_auc score', pr_auc(y_train, y_train_pre))\n",
    "    print('Validation pr_auc, score', pr_auc(y_val, y_val_pre))\n",
    "    print('Test pr_auc score', pr_auc(y_test, y_test_pre))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    159200\n",
      "           1       0.92      0.73      0.82       291\n",
      "\n",
      "    accuracy                           1.00    159491\n",
      "   macro avg       0.96      0.87      0.91    159491\n",
      "weighted avg       1.00      1.00      1.00    159491\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39800\n",
      "           1       0.93      0.71      0.81        73\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.96      0.86      0.90     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       0.87      0.68      0.76       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.93      0.84      0.88     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 0.8272628703866558\n",
      "Validation pr_auc, score 0.8207134339394506\n",
      "Test pr_auc score 0.7750836760325597\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split the training into training and validation sets\n",
    "# using stratified method, meaning the distributions of target classes are the same across train and validation.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42)\n",
    "\n",
    "lr_model(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is performing OK. Train and validation score is very similar. Recall score is less good as recision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With SMOTE and BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    159200\n",
      "           1       0.97      0.94      0.96     31840\n",
      "\n",
      "    accuracy                           0.99    191040\n",
      "   macro avg       0.98      0.97      0.97    191040\n",
      "weighted avg       0.99      0.99      0.99    191040\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     39800\n",
      "           1       0.22      0.85      0.35        73\n",
      "\n",
      "    accuracy                           0.99     39873\n",
      "   macro avg       0.61      0.92      0.67     39873\n",
      "weighted avg       1.00      0.99      1.00     39873\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     85315\n",
      "           1       0.17      0.83      0.28       128\n",
      "\n",
      "    accuracy                           0.99     85443\n",
      "   macro avg       0.59      0.91      0.64     85443\n",
      "weighted avg       1.00      0.99      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 0.961438344572604\n",
      "Validation pr_auc, score 0.5355097579138619\n",
      "Test pr_auc score 0.499400244013393\n"
     ]
    }
   ],
   "source": [
    "# with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42)\n",
    "\n",
    "# over sample the minority\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.2)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "lr_model(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    159200\n",
      "           1       0.99      0.99      0.99     31840\n",
      "\n",
      "    accuracy                           1.00    191040\n",
      "   macro avg       1.00      1.00      1.00    191040\n",
      "weighted avg       1.00      1.00      1.00    191040\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39800\n",
      "           1       0.44      0.73      0.55        73\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.72      0.86      0.77     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       0.39      0.71      0.50       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.69      0.85      0.75     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 0.9934263048007018\n",
      "Validation pr_auc, score 0.5840978282416536\n",
      "Test pr_auc score 0.5484818787842406\n"
     ]
    }
   ],
   "source": [
    "# with borderlineSMOTE\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42)\n",
    "\n",
    "# over sample the minority\n",
    "smote = BorderlineSMOTE(random_state=42, sampling_strategy=0.2)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "lr_model(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the validation and test score got significantly worse. The reason could be that the artificially generated minority samples aren't helping the model to be more generalised, and caused precision to dip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1455\n",
      "           1       0.98      0.90      0.94       291\n",
      "\n",
      "    accuracy                           0.98      1746\n",
      "   macro avg       0.98      0.95      0.96      1746\n",
      "weighted avg       0.98      0.98      0.98      1746\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     39800\n",
      "           1       0.19      0.89      0.31        73\n",
      "\n",
      "    accuracy                           0.99     39873\n",
      "   macro avg       0.59      0.94      0.65     39873\n",
      "weighted avg       1.00      0.99      1.00     39873\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     85315\n",
      "           1       0.17      0.85      0.28       128\n",
      "\n",
      "    accuracy                           0.99     85443\n",
      "   macro avg       0.58      0.92      0.64     85443\n",
      "weighted avg       1.00      0.99      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 0.9472824879041221\n",
      "Validation pr_auc, score 0.5397825421493746\n",
      "Test pr_auc score 0.5087191829549657\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42)\n",
    "\n",
    "# over sample the minority\n",
    "under = RandomUnderSampler(random_state=42, sampling_strategy=0.2)\n",
    "X_train, y_train = under.fit_resample(X_train, y_train)\n",
    "\n",
    "lr_model(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision calapsed under random under sampling, while recall is doing very well. <br> Overall undersampling looks very bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over and under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     79600\n",
      "           1       0.99      0.99      0.99     15920\n",
      "\n",
      "    accuracy                           1.00     95520\n",
      "   macro avg       0.99      1.00      1.00     95520\n",
      "weighted avg       1.00      1.00      1.00     95520\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39800\n",
      "           1       0.42      0.74      0.54        73\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.71      0.87      0.77     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       0.39      0.75      0.52       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.70      0.87      0.76     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 0.9925103122947067\n",
      "Validation pr_auc, score 0.5810387701629042\n",
      "Test pr_auc score 0.5719085708178954\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import sampler\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42)\n",
    "\n",
    "# over sample the minority\n",
    "X_train, y_train = sampler(X_train, y_train)\n",
    "\n",
    "lr_model(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks better than undersampling, but definitely not as good as without sampling. We will continue training the model without any resampling techniques. <br>\n",
    "\n",
    "We will also replace train-test split with RandomizedSearchCV to even out the randomness in the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a KNN to the training data with the following hyperparameters: <br>\n",
    "1. n_neighbors: 1-20\n",
    "2. metric: 'euclidean', 'manhattan']<br>\n",
    "We use pr_auc as scoring in random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "score = make_scorer(pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 41.1min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 123.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'n_neighbors': 20, 'metric': 'euclidean'}\n",
      "Best pr_auc score is: 0.8284858254052143\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "parameter_space = {\n",
    "    'n_neighbors': range(1, 21),\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "            }\n",
    "knn_search = RandomizedSearchCV(\n",
    "    estimator=knn,\n",
    "    param_distributions=parameter_space,\n",
    "    scoring=score,\n",
    "    n_iter=10,\n",
    "    cv=10,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "knn_search.fit(X, y)\n",
    "print('Best parameters are:', knn_search.best_params_)\n",
    "print('Best pr_auc score is:', knn_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result seems comparable to logistic regression. <br>\n",
    "Let's test on test set. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    199000\n",
      "           1       1.00      0.03      0.05       364\n",
      "\n",
      "    accuracy                           1.00    199364\n",
      "   macro avg       1.00      0.51      0.53    199364\n",
      "weighted avg       1.00      1.00      1.00    199364\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       1.00      0.04      0.08       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       1.00      0.52      0.54     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 0.5146240870142879\n",
      "Test pr_auc score 0.5202510280976792\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=20, metric='euclidean')\n",
    "knn.fit(X, y)\n",
    "y_pred = knn.predict(X)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "print(classification_report(y, y_pred))\n",
    "print('---------------------------------------------------------------------')\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print('---------------------------------------------------------------------')\n",
    "\n",
    "print('Train pr_auc score', pr_auc(y, y_pred))\n",
    "print('Test pr_auc score', pr_auc(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall score is terrible! The RandomSearchCV best param is not working as it should!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 54.0min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 166.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_depth': None, 'bootstrap': True}\n",
      "Best pr_auc score is: 0.8358577151012158\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "parameter_space = {\n",
    "                'n_estimators': [100, 500],\n",
    "                'max_depth': [5, 10, 20, None],\n",
    "                'min_samples_split': [2, 10],\n",
    "                'min_samples_leaf': [1, 5, 10],\n",
    "                'bootstrap': [True]\n",
    "            }\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=parameter_space,\n",
    "    scoring=score,\n",
    "    n_iter=10,\n",
    "    cv=10,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search.fit(X, y)\n",
    "print('Best parameters are:', rf_search.best_params_)\n",
    "print('Best pr_auc score is:', rf_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a slight increase in the pr_auc score. Let's fit the best model on the whole training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    199000\n",
      "           1       0.98      0.83      0.90       364\n",
      "\n",
      "    accuracy                           1.00    199364\n",
      "   macro avg       0.99      0.92      0.95    199364\n",
      "weighted avg       1.00      1.00      1.00    199364\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       0.87      0.76      0.81       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.94      0.88      0.91     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 0.9082448945889686\n",
      "Test pr_auc score 0.8160245944249699\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, \n",
    "                            min_samples_split=2,\n",
    "                           min_samples_leaf=5,\n",
    "                           max_depth=None,\n",
    "                           bootstrap=True)\n",
    "rf.fit(X, y)\n",
    "y_pred = rf.predict(X)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "print(classification_report(y, y_pred))\n",
    "print('---------------------------------------------------------------------')\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print('---------------------------------------------------------------------')\n",
    "\n",
    "print('Train pr_auc score', pr_auc(y, y_pred))\n",
    "print('Test pr_auc score', pr_auc(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an improvement from logistic regression. Recall is still less than precision by 10 percent point. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try XGBoost in attempt to improve the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    159200\n",
      "           1       1.00      1.00      1.00       291\n",
      "\n",
      "    accuracy                           1.00    159491\n",
      "   macro avg       1.00      1.00      1.00    159491\n",
      "weighted avg       1.00      1.00      1.00    159491\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39800\n",
      "           1       0.95      0.84      0.89        73\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.98      0.92      0.95     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 1.0\n",
      "Val pr_auc score 0.8945211969449921\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_train)\n",
    "y_val_pred = xgb.predict(X_val)\n",
    "\n",
    "print(classification_report(y_train, y_pred))\n",
    "print('---------------------------------------------------------------------')\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print('---------------------------------------------------------------------')\n",
    "\n",
    "print('Train pr_auc score', pr_auc(y_train, y_pred))\n",
    "print('Val pr_auc score', pr_auc(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "              validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a big increase in precision and recall scores. Acknowledging that there is still a big gap between train and validation, we are going to tune the hyperparameters. Have researched and tried to implement on sklearn's random search cv, but failed to find ways to incorperate early stopping. So we are going to implement without early stopping <br>\n",
    "\n",
    "We are going to look at some of these hyperparameters, as per explained in this helpful video https://www.youtube.com/watch?v=AvWfL1Us3Kg and xgboost documentation https://xgboost.readthedocs.io/en/latest/parameter.html:<br>\n",
    "\n",
    "1. **max_depth**: maxium depth of trees, usually range(2,30), default 3\n",
    "2. **subsample**: subsample ratio of training instances, usually range (0.1,1), default 1\n",
    "3. **colsample_bylevel**: subsample ratio of training features at each level, usually range (0.1,1), default 1\n",
    "4. **colsample_bytree**: subsample ratio of training features at each tree, usually range (0.1,1), default 1\n",
    "5. **min_child_weight**: minimum sum of instance weight (hessian) needed in a child, usually range(0,inf), default 1\n",
    "6. **reg_lambda**: L2 regularization term on weights, default 1, the higher the higher regularisation\n",
    "7. **learning_rate**: usually 0.01-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {'n_estimators': [100, 200, 500],\n",
    "               'max_depth': [3, 5, 10, 20],\n",
    "               'min_child_weight': [1, 5, 10],\n",
    "               'subsample': [0.5, 0.8, 1],\n",
    "               'colsample_bytree': [0.8, 1]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 55.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 20, 'bootstrap': True}\n",
      "Best pr_auc score is: -0.00029854944962667715\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_search = RandomizedSearchCV(estimator=xgb,\n",
    "                                param_distributions=param_space,\n",
    "                                scoring=score,\n",
    "                                n_iter=10,\n",
    "                                cv=10,\n",
    "                                verbose=2,\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1\n",
    "                               )\n",
    "xgb_search.fit(X, y)\n",
    "print('Best parameters are:', rf_search.best_params_)\n",
    "print('Best pr_auc score is:', rf_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note** the above code was run with the wrong param space, realised too late, not enough time to change. We presume the default is the best model and run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    199000\n",
      "           1       1.00      1.00      1.00       364\n",
      "\n",
      "    accuracy                           1.00    199364\n",
      "   macro avg       1.00      1.00      1.00    199364\n",
      "weighted avg       1.00      1.00      1.00    199364\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       0.91      0.78      0.84       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.89      0.92     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 1.0\n",
      "Test pr_auc score 0.8453343064701294\n"
     ]
    }
   ],
   "source": [
    "def fit_best_model(X, y, X_test, y_test):\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X, y)\n",
    "    y_pred = xgb.predict(X)\n",
    "    y_test_pred = xgb.predict(X_test)\n",
    "\n",
    "    print(classification_report(y, y_pred))\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print('---------------------------------------------------------------------')\n",
    "\n",
    "    print('Train pr_auc score', pr_auc(y, y_pred))\n",
    "    print('Test pr_auc score', pr_auc(y_test, y_test_pred))\n",
    "\n",
    "fit_best_model(X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using over and undersampling again with xgboost. From previous discussions, there is implication that sampling methods can be algorithm sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    199000\n",
      "           1       1.00      1.00      1.00     19900\n",
      "\n",
      "    accuracy                           1.00    218900\n",
      "   macro avg       1.00      1.00      1.00    218900\n",
      "weighted avg       1.00      1.00      1.00    218900\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       0.83      0.81      0.82       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.92      0.91      0.91     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 1.0\n",
      "Test pr_auc score 0.8223904445068642\n"
     ]
    }
   ],
   "source": [
    "# SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.1)\n",
    "X_train, y_train = smote.fit_resample(X, y)\n",
    "fit_best_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    199000\n",
      "           1       1.00      1.00      1.00     19900\n",
      "\n",
      "    accuracy                           1.00    218900\n",
      "   macro avg       1.00      1.00      1.00    218900\n",
      "weighted avg       1.00      1.00      1.00    218900\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       0.87      0.79      0.83       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.94      0.89      0.91     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 1.0\n",
      "Test pr_auc score 0.8300340776564291\n"
     ]
    }
   ],
   "source": [
    "# BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(random_state=42, sampling_strategy=0.1)\n",
    "X_train, y_train = smote.fit_resample(X, y)\n",
    "fit_best_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     99500\n",
      "           1       1.00      1.00      1.00     19900\n",
      "\n",
      "    accuracy                           1.00    119400\n",
      "   macro avg       1.00      1.00      1.00    119400\n",
      "weighted avg       1.00      1.00      1.00    119400\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       0.83      0.80      0.81       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.91      0.90      0.91     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 1.0\n",
      "Test pr_auc score 0.8132237945572329\n"
     ]
    }
   ],
   "source": [
    "# Over and under sample\n",
    "from preprocessing import sampler\n",
    "X_train, y_train = sampler(X, y)\n",
    "fit_best_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like over and under sampling didn't improve the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C is:  {'C': 100}\n",
      "Best score is: 0.5009129029814122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "param_space = {'C': [100, 10, 1.0, 0.1, 0.001]}\n",
    "svc = SVC()\n",
    "svc_search = RandomizedSearchCV(\n",
    "    estimator=svc,\n",
    "    param_distributions=param_space,\n",
    "    scoring=score,\n",
    "    n_iter=5,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "svc_search.fit(X, y)\n",
    "print('Best C is: ', svc_search.best_params_)\n",
    "print('Best score is:', svc_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC seems to predict badly, so we will go back to random forest again, and try out different scoring methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest with custom score randon search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount for fraud transaction amount 127.43673076923078\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('creditcard_train.csv')\n",
    "print('Average amount for fraud transaction amount', train_data[train_data['Class']==1]['Amount'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a credit card company would want to use the cost of recall and precision to set model. Here we wrote a function that takes in cost weights to calculate the scoring metric, rather than area under the curve. <br>\n",
    "\n",
    "The cost of low precision: genuine transactions blocked, the customer would be unhappy which could cost the credit card company labour cost to put it right, and loss of business.<br>\n",
    "\n",
    "The cost of low recall: fraud transaction going through, the credit card company will lose the transaction amount if the money can't be claimed back.<br>\n",
    "\n",
    "We could devise two kinds of metrics:<br>\n",
    "\n",
    "1. Static weighted metric: this metric is fixed and used cross all transactions regardless of the transaction amount.<br>\n",
    "2. Dynamic weighted metric: this metric is dynamic. It changes as the transaction amount changes. <br>\n",
    "\n",
    "Here we use custome static metric for simplification, and presume, average genuine transaction blocked would cost credit card company £50, and average fraud transaction going through £127. <br>\n",
    "\n",
    "So the weights are precision: 50/(50+127)=0.28, and recall 0.72<br>\n",
    "\n",
    "We also zoom in on the random forest parameter search area to where the best performing one was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 36.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 20, 'bootstrap': True}\n",
      "Best custom_metric score is: -0.00029854944962667715\n"
     ]
    }
   ],
   "source": [
    "from metrics import custom_metric\n",
    "\n",
    "score = make_scorer(custom_metric)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "parameter_space = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [20, None],\n",
    "                'min_samples_split': [1, 2, 3, 5],\n",
    "                'min_samples_leaf': [2, 5, 8],\n",
    "                'bootstrap': [True]\n",
    "            }\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=parameter_space,\n",
    "    scoring=score,\n",
    "    n_iter=10,\n",
    "    cv=10,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search.fit(X, y)\n",
    "print('Best parameters are:', rf_search.best_params_)\n",
    "print('Best custom_metric score is:', rf_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the model again with the best parameters. And test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    199000\n",
      "           1       0.99      0.85      0.92       364\n",
      "\n",
      "    accuracy                           1.00    199364\n",
      "   macro avg       1.00      0.93      0.96    199364\n",
      "weighted avg       1.00      1.00      1.00    199364\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85315\n",
      "           1       0.89      0.76      0.82       128\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.94      0.88      0.91     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Train pr_auc score 0.9227544782885767\n",
      "Test pr_auc score 0.8240417859283999\n",
      "The custom score for train data is -0.00028089324050480525\n",
      "The custom score for train data is -0.0005032594829301406\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, \n",
    "                            min_samples_split=5,\n",
    "                           min_samples_leaf=2,\n",
    "                           max_depth=20,\n",
    "                           bootstrap=True)\n",
    "rf.fit(X, y)\n",
    "y_pred = rf.predict(X)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "print(classification_report(y, y_pred))\n",
    "print('---------------------------------------------------------------------')\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print('---------------------------------------------------------------------')\n",
    "\n",
    "print('Train pr_auc score', pr_auc(y, y_pred))\n",
    "print('Test pr_auc score', pr_auc(y_test, y_test_pred))\n",
    "print('The custom score for train data is', custom_metric(y, y_pred))\n",
    "print('The custom score for test data is', custom_metric(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
